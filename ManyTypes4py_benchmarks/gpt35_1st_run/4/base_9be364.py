from __future__ import annotations
import abc
import asyncio
import threading
from contextlib import AsyncExitStack
from functools import partial
from typing import TYPE_CHECKING, Any, Callable, Dict, Generic, List, Optional, Set, Type, Union
from uuid import UUID, uuid4
import anyio
import anyio.abc
import httpx
from importlib_metadata import distributions
from pydantic import BaseModel, Field, PrivateAttr, field_validator
from pydantic.json_schema import GenerateJsonSchema
from typing_extensions import Literal, Self, TypeVar
import prefect
from prefect._internal.schemas.validators import return_v_or_none
from prefect.client.base import ServerType
from prefect.client.orchestration import PrefectClient, get_client
from prefect.client.schemas.actions import WorkPoolCreate, WorkPoolUpdate
from prefect.client.schemas.objects import Integration, StateType, WorkerMetadata, WorkPool
from prefect.client.utilities import inject_client
from prefect.events import Event, RelatedResource, emit_event
from prefect.events.related import object_as_related_resource, tags_as_related_resources
from prefect.exceptions import Abort, ObjectNotFound
from prefect.logging.loggers import PrefectLogAdapter, flow_run_logger, get_worker_logger
from prefect.plugins import load_prefect_collections
from prefect.settings import PREFECT_API_URL, PREFECT_TEST_MODE, PREFECT_WORKER_HEARTBEAT_SECONDS, PREFECT_WORKER_PREFETCH_SECONDS, PREFECT_WORKER_QUERY_SECONDS, get_current_settings
from prefect.states import Crashed, Pending, exception_to_failed_state
from prefect.types import KeyValueLabels
from prefect.types._datetime import DateTime
from prefect.utilities.dispatch import get_registry_for_type, register_base_type
from prefect.utilities.engine import propose_state
from prefect.utilities.services import critical_service_loop
from prefect.utilities.slugify import slugify
from prefect.utilities.templating import apply_values, resolve_block_document_references, resolve_variables
from prefect.utilities.urls import url_for

if TYPE_CHECKING:
    from prefect.client.schemas.objects import Flow, FlowRun
    from prefect.client.schemas.responses import DeploymentResponse, WorkerFlowRunResponse

class BaseJobConfiguration(BaseModel):
    command: Optional[str] = Field(default=None, description='The command to use when starting a flow run. In most cases, this should be left blank and the command will be automatically generated by the worker.')
    env: Dict[str, str] = Field(default_factory=dict, title='Environment Variables', description='Environment variables to set when starting a flow run.')
    labels: Dict[str, Any] = Field(default_factory=dict, description='Labels applied to infrastructure created by the worker using this job configuration.')
    name: Optional[str] = Field(default=None, description='Name given to infrastructure created by the worker using this job configuration.')
    _related_objects: Dict[str, Any] = PrivateAttr(default_factory=dict)

    @property
    def is_using_a_runner(self) -> bool:
        return self.command is not None and 'prefect flow-run execute' in self.command

    @field_validator('command')
    @classmethod
    def _coerce_command(cls, v: Any) -> Optional[str]:
        return return_v_or_none(v)

    @field_validator('env', mode='before')
    @classmethod
    def _coerce_env(cls, v: Dict[str, Any]) -> Dict[str, str]:
        return {k: str(v) if v is not None else None for k, v in v.items()}

    @staticmethod
    def _get_base_config_defaults(variables: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        defaults = dict()
        for variable_name, attrs in variables.items():
            if 'default' in attrs and attrs.get('default') is not None:
                defaults[variable_name] = attrs['default']
        return defaults

    @classmethod
    @inject_client
    async def from_template_and_values(cls, base_job_template: Dict[str, Any], values: Dict[str, Any], client=None) -> BaseJobConfiguration:
        base_config = base_job_template['job_configuration']
        variables_schema = base_job_template['variables']
        variables = cls._get_base_config_defaults(variables_schema.get('properties', {}))
        if variables.get('env'):
            base_config['env'] = variables.get('env')
        variables.update(values)
        if isinstance(base_config.get('env'), dict) and (deployment_env := variables.get('env')):
            base_config['env'] = base_config.get('env') | deployment_env
        populated_configuration = apply_values(template=base_config, values=variables)
        populated_configuration = await resolve_block_document_references(template=populated_configuration, client=client)
        populated_configuration = await resolve_variables(template=populated_configuration, client=client)
        return cls(**populated_configuration)

    @classmethod
    def json_template(cls) -> Dict[str, str]:
        configuration = {}
        properties = cls.model_json_schema()['properties']
        for k, v in properties.items():
            if v.get('template'):
                template = v['template']
            else:
                template = '{{ ' + k + ' }}'
            configuration[k] = template
        return configuration

    def prepare_for_flow_run(self, flow_run: FlowRun, deployment=None, flow=None) -> None:
        self._related_objects = {'deployment': deployment, 'flow': flow, 'flow-run': flow_run}
        if deployment is not None:
            deployment_labels = self._base_deployment_labels(deployment)
        else:
            deployment_labels = {}
        if flow is not None:
            flow_labels = self._base_flow_labels(flow)
        else:
            flow_labels = {}
        env = {**self._base_environment(), **self._base_flow_run_environment(flow_run), **(self.env if isinstance(self.env, dict) else {})}
        self.env = {key: value for key, value in env.items() if value is not None}
        self.labels = {**self._base_flow_run_labels(flow_run), **deployment_labels, **flow_labels, **self.labels}
        self.name = self.name or flow_run.name
        self.command = self.command or self._base_flow_run_command()

    @staticmethod
    def _base_flow_run_command() -> str:
        return 'prefect flow-run execute'

    @staticmethod
    def _base_flow_run_labels(flow_run: FlowRun) -> Dict[str, str]:
        return {'prefect.io/flow-run-id': str(flow_run.id), 'prefect.io/flow-run-name': flow_run.name, 'prefect.io/version': prefect.__version__}

    @classmethod
    def _base_environment(cls) -> Dict[str, str]:
        return get_current_settings().to_environment_variables(exclude_unset=True)

    @staticmethod
    def _base_flow_run_environment(flow_run: FlowRun) -> Dict[str, str]:
        return {'PREFECT__FLOW_RUN_ID': str(flow_run.id)}

    @staticmethod
    def _base_deployment_labels(deployment: DeploymentResponse) -> Dict[str, str]:
        labels = {'prefect.io/deployment-id': str(deployment.id), 'prefect.io/deployment-name': deployment.name}
        if deployment.updated is not None:
            labels['prefect.io/deployment-updated'] = deployment.updated.in_timezone('utc').to_iso8601_string()
        return labels

    @staticmethod
    def _base_flow_labels(flow: Flow) -> Dict[str, str]:
        return {'prefect.io/flow-id': str(flow.id), 'prefect.io/flow-name': flow.name}

    def _related_resources(self) -> List[RelatedResource]:
        tags = set()
        related = []
        for kind, obj in self._related_objects.items():
            if obj is None:
                continue
            if hasattr(obj, 'tags'):
                tags.update(obj.tags)
            related.append(object_as_related_resource(kind=kind, role=kind, object=obj))
        return related + tags_as_related_resources(tags)

class BaseVariables(BaseModel):
    name: Optional[str] = Field(default=None, description='Name given to infrastructure created by a worker.')
    env: Dict[str, str] = Field(default_factory=dict, title='Environment Variables', description='Environment variables to set when starting a flow run.')
    labels: Dict[str, Any] = Field(default_factory=dict, description='Labels applied to infrastructure created by a worker.')
    command: Optional[str] = Field(default=None, description='The command to use when starting a flow run. In most cases, this should be left blank and the command will be automatically generated by the worker.')

    @classmethod
    def model_json_schema(cls, by_alias: bool = True, ref_template: str = '#/definitions/{model}', schema_generator: GenerateJsonSchema = GenerateJsonSchema, mode: str = 'validation') -> Dict[str, Any]:
        schema = super().model_json_schema(by_alias, ref_template, schema_generator, mode)
        if '$defs' in schema:
            schema['definitions'] = schema.pop('$defs')
        if 'additionalProperties' in schema:
            schema.pop('additionalProperties')
        for _, definition in schema.get('definitions', {}).items():
            if 'additionalProperties' in definition:
                definition.pop('additionalProperties')
        return schema

class BaseWorkerResult(BaseModel, abc.ABC):

    def __bool__(self) -> bool:
        return self.status_code == 0

C = TypeVar('C', bound=BaseJobConfiguration)
V = TypeVar('V', bound=BaseVariables)
R = TypeVar('R', bound=BaseWorkerResult)

@register_base_type
class BaseWorker(abc.ABC, Generic[C, V, R]):
    job_configuration: Type[BaseJobConfiguration] = BaseJobConfiguration
    job_configuration_variables: Optional[Type[BaseVariables]] = None
    _documentation_url: str = ''
    _logo_url: str = ''
    _description: str = ''

    def __init__(self, work_pool_name: str, work_queues: Optional[List[str]] = None, name: Optional[str] = None, prefetch_seconds: Optional[int] = None, create_pool_if_not_found: bool = True, limit: Optional[int] = None, heartbeat_interval_seconds: Optional[int] = None, *, base_job_template: Optional[Dict[str, Any]] = None) -> None:
        """
        Base class for all Prefect workers.

        Args:
            name: The name of the worker. If not provided, a random one
                will be generated. If provided, it cannot contain '/' or '%'.
                The name is used to identify the worker in the UI; if two
                processes have the same name, they will be treated as the same
                worker.
            work_pool_name: The name of the work pool to poll.
            work_queues: A list of work queues to poll. If not provided, all
                work queue in the work pool will be polled.
            prefetch_seconds: The number of seconds to prefetch flow runs for.
            create_pool_if_not_found: Whether to create the work pool
                if it is not found. Defaults to `True`, but can be set to `False` to
                ensure that work pools are not created accidentally.
            limit: The maximum number of flow runs this worker should be running at
                a given time.
            heartbeat_interval_seconds: The number of seconds between worker heartbeats.
            base_job_template: If creating the work pool, provide the base job
                template to use. Logs a warning if the pool already exists.
        """
        if name and ('/' in name or '%' in name):
            raise ValueError("Worker name cannot contain '/' or '%'")
        self.name = name or f'{self.__class__.__name__} {uuid4()}'
        self._started_event = None
        self.backend_id = None
        self._logger = get_worker_logger(self)
        self.is_setup = False
        self._create_pool_if_not_found = create_pool_if_not_found
        self._base_job_template = base_job_template
        self._work_pool_name = work_pool_name
        self._work_queues = set(work_queues) if work_queues else set()
        self._prefetch_seconds = prefetch_seconds or PREFECT_WORKER_PREFETCH_SECONDS.value()
        self.heartbeat_interval_seconds = heartbeat_interval_seconds or PREFECT_WORKER_HEARTBEAT_SECONDS.value()
        self._work_pool = None
        self._exit_stack = AsyncExitStack()
        self._runs_task_group = None
        self._client = None
        self._last_polled_time = DateTime.now('utc')
        self._limit = limit
        self._limiter = None
        self._submitting_flow_run_ids = set()
        self._cancelling_flow_run_ids = set()
        self._scheduled_task_scopes = set()
        self._worker_metadata_sent = False

    @classmethod
    def get_documentation_url(cls) -> str:
        return cls._documentation_url

    @classmethod
    def get_logo_url(cls) -> str:
        return cls._logo_url

    @classmethod
    def get_description(cls) -> str:
        return cls._description

    @classmethod
    def get_default_base_job_template(cls) -> Dict[str, Any]:
        if cls.job_configuration_variables is None:
            schema = cls.job_configuration.model_json_schema()
            for key, value in schema['properties'].items():
                if isinstance(value, dict):
                    schema['properties'][key].pop('template', None)
            variables_schema = schema
        else:
            variables_schema = cls.job_configuration_variables.model_json_schema()
        variables_schema.pop('title', None)
        return {'job_configuration': cls.job_configuration.json_template(), 'variables': variables_schema}

    @staticmethod
    def get_worker_class_from_type(type: str) -> Optional[Type[BaseWorker]]:
        """
        Returns the worker class for a given worker type. If the worker type
        is not recognized, returns None.
        """
        load_prefect_collections()
        worker_registry = get_registry_for_type(BaseWorker)
        if worker_registry is not None:
            return worker_registry.get(type)

    @staticmethod
    def get_all_available_worker_types() -> List[str]:
        """
        Returns all worker types available in the local registry.
        """
        load_prefect_collections()
        worker_registry = get_registry_for_type(BaseWorker)
        if worker_registry is not None:
            return list(worker_registry.keys())
        return []

    def get_name_slug(self) -> str:
        return slugify(self.name)

    def get_flow_run_logger(self, flow_run: FlowRun) -> PrefectLogAdapter:
        extra = {'worker_name': self.name, 'work_pool_name': self._work_pool_name if self._work_pool else '<unknown>', 'work_pool_id': str(getattr(self._work_pool, 'id', 'unknown'))}
        if self.backend_id:
            extra['worker_id'] = str(self.backend_id)
        return flow_run_logger(flow_run=flow_run).getChild('worker', extra=extra)

    async def start(self, run_once: bool = False, with_healthcheck: bool = False, printer: Callable[[str], None] = print) -> None:
        """
        Starts the worker and runs the main worker loops.

        By default, the worker will run loops to poll for scheduled/cancelled flow
        runs and sync with the Prefect API server.

        If `run_once` is set, the worker will only run each loop once and then return.

        If `with_healthcheck` is set, the worker will start a healthcheck server which
        can be used to determine if the worker is still polling for flow runs and restart
        the worker if necessary.

        Args:
            run_once: If set, the worker will only run each loop once then return.
            with_healthcheck: If set, the worker will start a healthcheck server.
            printer: A `print`-like function where logs will be reported.
        """
        healthcheck_server = None
        healthcheck_thread = None
        try:
            async with self as worker:
                await worker.sync_with_backend()
                async with anyio.create_task_group() as loops_task_group:
                    loops_task_group.start_soon(partial(critical_service_loop, workload=worker.get_and_submit_flow_runs, interval=PREFECT_WORKER_QUERY_SECONDS.value(), run_once=run_once, jitter_range=0.3, backoff=4))
                    loops_task_group.start_soon(partial(critical_service_loop, workload=worker.sync_with_backend, interval=worker.heartbeat_interval_seconds, run_once=run_once, jitter_range=0.3, backoff=4))
                    self._started_event = await self._emit_worker_started_event()
                    if with_healthcheck:
                        from prefect.workers.server import build_healthcheck_server
                        healthcheck_server = build_healthcheck_server(worker=worker, query_interval_seconds=PREFECT_WORKER_QUERY_SECONDS.value())
                        healthcheck_thread = threading.Thread(name='healthcheck-server-thread', target=healthcheck_server.run, daemon=True)
                        healthcheck_thread.start()
                    printer(f'Worker {worker.name!r} started!')
        finally:
            if healthcheck_server and healthcheck_thread:
                self._logger.debug('Stopping healthcheck server...')
                healthcheck_server.should_exit = True
                healthcheck_thread.join()
                self._logger.debug('Healthcheck server stopped.')
        printer(f'Worker {worker.name!r} stopped!')

    @abc.abstractmethod
    async def run(self, flow_run: FlowRun, configuration: BaseJobConfiguration, task_status: Optional[Any] = None) -> Any:
        """
        Runs a given flow run on the current worker.
        """
        raise NotImplementedError('Workers must implement a method for running submitted flow runs')

    @classmethod
    def __dispatch_key__(cls) -> Optional[str]:
        if cls.__name__ == 'BaseWorker':
            return None
        return cls.type

    async def setup(self) -> None:
        """Prepares the worker to run."""
        self._logger.debug('Setting up worker...')
        self._runs_task_group = anyio.create_task_group()
        self._limiter = anyio.CapacityLimiter(self._limit) if self._limit is not None else None
        if not PREFECT_TEST_MODE and (not PREFECT_API_URL.value()):
            raise ValueError('`PREFECT_API_URL` must be set to start a Worker.')
        self._client = get_client()
        await self._exit_stack.enter_async_context(self._client)
        await self._exit_stack.enter_async_context(self._runs_task_group)
        self.is_setup = True

    async def teardown(self, *exc_info) -> None:
        """Cleans up resources after the worker is stopped."""
        self._logger.debug('Tearing down worker...')
        self.is_setup = False
        for scope in self._scheduled_task_scopes:
            scope.cancel()
        if self._started_event:
            try:
                await self._emit_worker_stopped_event(self._started_event)
            except Exception:
                self._logger.exception('Failed to emit worker stopped event')
        await self._exit_stack.__aexit__(*exc_info)
        self._runs_task_group = None
        self._client = None

    def is_worker_still_polling(self, query_interval_seconds: int) -> bool:
        """
        This method is invoked by a webserver healthcheck handler
        and returns a boolean indicating if the worker has recorded a
        scheduled flow run poll within a variable amount of time.

        The `query_interval_seconds` is the same value that is used by
        the loop services - we will evaluate if the _last