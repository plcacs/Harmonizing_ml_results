from __future__ import annotations
import abc
import asyncio
import threading
from contextlib import AsyncExitStack
from functools import partial
from typing import TYPE_CHECKING, Any, Callable, Dict, Generic, List, Optional, Set, Type, Union
from uuid import UUID, uuid4
import anyio
import anyio.abc
import httpx
from importlib_metadata import distributions
from pydantic import BaseModel, Field, PrivateAttr, field_validator
from pydantic.json_schema import GenerateJsonSchema
from typing_extensions import Literal, Self, TypeVar
import prefect
from prefect._internal.schemas.validators import return_v_or_none
from prefect.client.base import ServerType
from prefect.client.orchestration import PrefectClient, get_client
from prefect.client.schemas.actions import WorkPoolCreate, WorkPoolUpdate
from prefect.client.schemas.objects import Integration, StateType, WorkerMetadata, WorkPool
from prefect.client.utilities import inject_client
from prefect.events import Event, RelatedResource, emit_event
from prefect.events.related import object_as_related_resource, tags_as_related_resources
from prefect.exceptions import Abort, ObjectNotFound
from prefect.logging.loggers import PrefectLogAdapter, flow_run_logger, get_worker_logger
from prefect.plugins import load_prefect_collections
from prefect.settings import PREFECT_API_URL, PREFECT_TEST_MODE, PREFECT_WORKER_HEARTBEAT_SECONDS, PREFECT_WORKER_PREFETCH_SECONDS, PREFECT_WORKER_QUERY_SECONDS, get_current_settings
from prefect.states import Crashed, Pending, exception_to_failed_state
from prefect.types import KeyValueLabels
from prefect.types._datetime import DateTime
from prefect.utilities.dispatch import get_registry_for_type, register_base_type
from prefect.utilities.engine import propose_state
from prefect.utilities.services import critical_service_loop
from prefect.utilities.slugify import slugify
from prefect.utilities.templating import apply_values, resolve_block_document_references, resolve_variables
from prefect.utilities.urls import url_for
if TYPE_CHECKING:
    from prefect.client.schemas.objects import Flow, FlowRun
    from prefect.client.schemas.responses import DeploymentResponse, WorkerFlowRunResponse

class BaseJobConfiguration(BaseModel):
    command: Optional[str] = Field(default=None, description='The command to use when starting a flow run. In most cases, this should be left blank and the command will be automatically generated by the worker.')
    env: Dict[str, str] = Field(default_factory=dict, title='Environment Variables', description='Environment variables to set when starting a flow run.')
    labels: Dict[str, Any] = Field(default_factory=dict, description='Labels applied to infrastructure created by the worker using this job configuration.')
    name: Optional[str] = Field(default=None, description='Name given to infrastructure created by the worker using this job configuration.')
    _related_objects: Dict[str, Any] = PrivateAttr(default_factory=dict)

    @property
    def is_using_a_runner(self) -> bool:
        return self.command is not None and 'prefect flow-run execute' in self.command

    @field_validator('command')
    @classmethod
    def _coerce_command(cls, v: str) -> Optional[str]:
        return return_v_or_none(v)

    @field_validator('env', mode='before')
    @classmethod
    def _coerce_env(cls, v: Dict[str, Any]) -> Dict[str, str]:
        return {k: str(v) if v is not None else None for k, v in v.items()}

    @staticmethod
    def _get_base_config_defaults(variables: Dict[str, Any]) -> Dict[str, Any]:
        defaults = dict()
        for variable_name, attrs in variables.items():
            if 'default' in attrs and attrs.get('default') is not None:
                defaults[variable_name] = attrs['default']
        return defaults

    @classmethod
    @inject_client
    async def from_template_and_values(cls, base_job_template: Dict[str, Any], values: Dict[str, Any], client=None) -> BaseJobConfiguration:
        base_config = base_job_template['job_configuration']
        variables_schema = base_job_template['variables']
        variables = cls._get_base_config_defaults(variables_schema.get('properties', {}))
        if variables.get('env'):
            base_config['env'] = variables.get('env')
        variables.update(values)
        if isinstance(base_config.get('env'), dict) and (deployment_env := variables.get('env')):
            base_config['env'] = base_config.get('env') | deployment_env
        populated_configuration = apply_values(template=base_config, values=variables)
        populated_configuration = await resolve_block_document_references(template=populated_configuration, client=client)
        populated_configuration = await resolve_variables(template=populated_configuration, client=client)
        return cls(**populated_configuration)

    @classmethod
    def json_template(cls) -> Dict[str, str]:
        configuration = {}
        properties = cls.model_json_schema()['properties']
        for k, v in properties.items():
            if v.get('template'):
                template = v['template']
            else:
                template = '{{ ' + k + ' }}'
            configuration[k] = template
        return configuration

    def prepare_for_flow_run(self, flow_run: FlowRun, deployment=None, flow=None):
        self._related_objects = {'deployment': deployment, 'flow': flow, 'flow-run': flow_run}
        if deployment is not None:
            deployment_labels = self._base_deployment_labels(deployment)
        else:
            deployment_labels = {}
        if flow is not None:
            flow_labels = self._base_flow_labels(flow)
        else:
            flow_labels = {}
        env = {**self._base_environment(), **self._base_flow_run_environment(flow_run), **(self.env if isinstance(self.env, dict) else {})}
        self.env = {key: value for key, value in env.items() if value is not None}
        self.labels = {**self._base_flow_run_labels(flow_run), **deployment_labels, **flow_labels, **self.labels}
        self.name = self.name or flow_run.name
        self.command = self.command or self._base_flow_run_command()

    @staticmethod
    def _base_flow_run_command() -> str:
        return 'prefect flow-run execute'

    @staticmethod
    def _base_flow_run_labels(flow_run: FlowRun) -> Dict[str, str]:
        return {'prefect.io/flow-run-id': str(flow_run.id), 'prefect.io/flow-run-name': flow_run.name, 'prefect.io/version': prefect.__version__}

    @classmethod
    def _base_environment(cls) -> Dict[str, str]:
        return get_current_settings().to_environment_variables(exclude_unset=True)

    @staticmethod
    def _base_flow_run_environment(flow_run: FlowRun) -> Dict[str, str]:
        return {'PREFECT__FLOW_RUN_ID': str(flow_run.id)}

    @staticmethod
    def _base_deployment_labels(deployment: Deployment) -> Dict[str, str]:
        labels = {'prefect.io/deployment-id': str(deployment.id), 'prefect.io/deployment-name': deployment.name}
        if deployment.updated is not None:
            labels['prefect.io/deployment-updated'] = deployment.updated.in_timezone('utc').to_iso8601_string()
        return labels

    @staticmethod
    def _base_flow_labels(flow: Flow) -> Dict[str, str]:
        return {'prefect.io/flow-id': str(flow.id), 'prefect.io/flow-name': flow.name}

    def _related_resources(self) -> List[RelatedResource]:
        tags = set()
        related = []
        for kind, obj in self._related_objects.items():
            if obj is None:
                continue
            if hasattr(obj, 'tags'):
                tags.update(obj.tags)
            related.append(object_as_related_resource(kind=kind, role=kind, object=obj))
        return related + tags_as_related_resources(tags)

class BaseVariables(BaseModel):
    name: Optional[str] = Field(default=None, description='Name given to infrastructure created by a worker.')
    env: Dict[str, str] = Field(default_factory=dict, title='Environment Variables', description='Environment variables to set when starting a flow run.')
    labels: Dict[str, Any] = Field(default_factory=dict, description='Labels applied to infrastructure created by a worker.')
    command: Optional[str] = Field(default=None, description='The command to use when starting a flow run. In most cases, this should be left blank and the command will be automatically generated by the worker.')

    @classmethod
    def model_json_schema(cls, by_alias: bool = True, ref_template: str = '#/definitions/{model}', schema_generator: GenerateJsonSchema = GenerateJsonSchema, mode: str = 'validation') -> Dict[str, Any]:
        schema = super().model_json_schema(by_alias, ref_template, schema_generator, mode)
        if '$defs' in schema:
            schema['definitions'] = schema.pop('$defs')
        if 'additionalProperties' in schema:
            schema.pop('additionalProperties')
        for _, definition in schema.get('definitions', {}).items():
            if 'additionalProperties' in definition:
                definition.pop('additionalProperties')
        return schema

class BaseWorkerResult(BaseModel, abc.ABC):

    def __bool__(self) -> bool:
        return self.status_code == 0

C = TypeVar('C', bound=BaseJobConfiguration)
V = TypeVar('V', bound=BaseVariables)
R = TypeVar('R', bound=BaseWorkerResult)

@register_base_type
class BaseWorker(abc.ABC, Generic[C, V, R]):
    job_configuration: Type[BaseJobConfiguration] = BaseJobConfiguration
    job_configuration_variables: Optional[Type[BaseVariables]] = None
    _documentation_url: str = ''
    _logo_url: str = ''
    _description: str = ''

    def __init__(self, work_pool_name: str, work_queues: Optional[List[str]] = None, name: Optional[str] = None, prefetch_seconds: Optional[int] = None, create_pool_if_not_found: bool = True, limit: Optional[int] = None, heartbeat_interval_seconds: Optional[int] = None, *, base_job_template: Optional[Dict[str, Any]] = None):
        ...

    @classmethod
    def get_documentation_url(cls) -> str:
        return cls._documentation_url

    @classmethod
    def get_logo_url(cls) -> str:
        return cls._logo_url

    @classmethod
    def get_description(cls) -> str:
        return cls._description

    @classmethod
    def get_default_base_job_template(cls) -> Dict[str, Any]:
        ...

    @staticmethod
    def get_worker_class_from_type(type: str) -> Optional[Type[BaseWorker]]:
        ...

    @staticmethod
    def get_all_available_worker_types() -> List[str]:
        ...

    def get_name_slug(self) -> str:
        return slugify(self.name)

    def get_flow_run_logger(self, flow_run: FlowRun) -> PrefectLogAdapter:
        ...

    async def start(self, run_once: bool = False, with_healthcheck: bool = False, printer: Callable[[str], None] = print) -> None:
        ...

    @abc.abstractmethod
    async def run(self, flow_run: FlowRun, configuration: C, task_status: Optional[Any] = None) -> R:
        ...

    @classmethod
    def __dispatch_key__(cls) -> Optional[str]:
        ...

    async def setup(self) -> None:
        ...

    async def teardown(self, *exc_info) -> None:
        ...

    def is_worker_still_polling(self, query_interval_seconds: int) -> bool:
        ...

    async def get_and_submit_flow_runs(self) -> List[FlowRun]:
        ...

    async def _update_local_work_pool_info(self) -> None:
        ...

    async def _worker_metadata(self) -> Optional[WorkerMetadata]:
        ...

    async def _send_worker_heartbeat(self) -> Optional[UUID]:
        ...

    async def sync_with_backend(self) -> None:
        ...

    def _should_get_worker_id(self) -> bool:
        ...

    async def _get_scheduled_flow_runs(self) -> List[WorkerFlowRunResponse]:
        ...

    async def _submit_scheduled_flow_runs(self, flow_run_response: List[WorkerFlowRunResponse]) -> List[FlowRun]:
        ...

    async def _check_flow_run(self, flow_run: FlowRun) -> None:
        ...

    async def _submit_run(self, flow_run: FlowRun) -> None:
        ...

    async def _submit_run_and_capture_errors(self, flow_run: FlowRun, task_status: Optional[Any] = None) -> Any:
        ...

    def _release_limit_slot(self, flow_run_id: UUID) -> None:
        ...

    def get_status(self) -> Dict[str, Any]:
        ...

    async def _get_configuration(self, flow_run: FlowRun, deployment: Optional[Deployment] = None) -> BaseJobConfiguration:
        ...

    async def _propose_pending_state(self, flow_run: FlowRun) -> bool:
        ...

    async def _propose_failed_state(self, flow_run: FlowRun, exc: Exception) -> None:
        ...

    async def _propose_crashed_state(self, flow_run: FlowRun, message: str) -> None:
        ...

    async def _mark_flow_run_as_cancelled(self, flow_run: FlowRun, state_updates: Optional[Dict[str, Any]] = None) -> None:
        ...

    async def _set_work_pool_template(self, work_pool: WorkPool, job_template: Dict[str, Any]) -> None:
        ...

    async def _schedule_task(self, __in_seconds: int, fn: Callable, *args, **kwargs) -> None:
        ...

    async def _give_worker_labels_to_flow_run(self, flow_run_id: UUID) -> None:
        ...

    async def __aenter__(self) -> BaseWorker:
        ...

    async def __aexit__(self, *exc_info) -> None:
        ...

    def __repr__(self) -> str:
        ...

    def _event_resource(self) -> Dict[str, str]:
        ...

    def _event_related_resources(self, configuration: Optional[BaseJobConfiguration] = None, include_self: bool = False) -> List[RelatedResource]:
        ...

    def _emit_flow_run_submitted_event(self, configuration: BaseJobConfiguration) -> Event:
        ...

    def _emit_flow_run_executed_event(self, result: BaseWorkerResult, configuration: BaseJobConfiguration, submitted_event: Event) -> None:
        ...

    async def _emit_worker_started_event(self) -> Event:
        ...

    async def _emit_worker_stopped_event(self, started_event: Event) -> None:
        ...
