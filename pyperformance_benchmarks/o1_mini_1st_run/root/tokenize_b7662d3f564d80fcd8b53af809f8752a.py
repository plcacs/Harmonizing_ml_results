__author__ = 'Ka-Ping Yee <ping@lfw.org>'
__credits__ = 'GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, Skip Montanaro'
import string
import re
from codecs import BOM_UTF8, LookupError, lookup
from lib2to3.pgen2.token import (
    ASYNC,
    AWAIT,
    COMMENT,
    DEDENT,
    ERRORTOKEN,
    INDENT,
    NEWLINE,
    NL,
    NAME,
    OP,
    NUMBER,
    STRING,
    ENDMARKER,
    tok_name,
)
from typing import (
    Callable,
    Generator,
    Iterator,
    List,
    Optional,
    Set,
    Tuple,
    Union,
)
from . import token

__all__: List[str] = [x for x in dir(token) if x[0] != '_'] + ['tokenize', 'generate_tokens', 'untokenize']
del token

try:
    bytes
except NameError:
    bytes = str

def group(*choices: str) -> str:
    return '(' + '|'.join(choices) + ')'

def any_(*choices: str) -> str:
    return group(*choices) + '*'

def maybe(*choices: str) -> str:
    return group(*choices) + '?'

def _combinations(*l: str) -> Set[str]:
    return set(x + y for x in l for y in (l + ('',)) if x.casefold() != y.casefold())

Whitespace: str = r'[ \f\t]*'
Comment: str = r'#[^\r\n]*'
Ignore: str = r'(?:' + Whitespace + any_(r'\\\r?\n' + Whitespace) + ')' + maybe(Comment)
Name: str = r'\w+'
Binnumber: str = r'0[bB]_?[01]+(?:_[01]+)*'
Hexnumber: str = r'0[xX]_?[\da-fA-F]+(?:_[\da-fA-F]+)*[lL]?'
Octnumber: str = r'0[oO]?_?[0-7]+(?:_[0-7]+)*[lL]?'
Decnumber: str = group(r'[1-9]\d*(?:_\d+)*[lL]?', r'0[lL]?')
Intnumber: str = group(Binnumber, Hexnumber, Octnumber, Decnumber)
Exponent: str = r'[eE][-+]?\d+(?:_\d+)*'
Pointfloat: str = group(r'\d+(?:_\d+)*\.(?:\d+(?:_\d+)*)?', r'\.\d+(?:_\d+)*') + maybe(Exponent)
Expfloat: str = r'\d+(?:_\d+)*' + Exponent
Floatnumber: str = group(Pointfloat, Expfloat)
Imagnumber: str = group(r'\d+(?:_\d+)*[jJ]', Floatnumber + r'[jJ]')
Number: str = group(Imagnumber, Floatnumber, Intnumber)
Single: str = r"[^'\\]*(?:\\.[^'\\]*)*'"
Double: str = r'[^"\\]*(?:\\.[^"\\]*)*"'
Single3: str = r"[^'\\]*(?:(?:\\.|'(?!''))[^'\\]*)*'''"
Double3: str = r'[^"\\]*(?:(?:\\.|"(?!""))[^"\\]*)*"""'
_litprefix: str = r'(?:[uUrRbBfF]|[rR][fFbB]|[fFbBuU][rR])?'
Triple: str = group(_litprefix + "'''", _litprefix + '"""')
String: str = group(
    _litprefix + r"'[^\\n'\\]*(?:\\.[^\\n'\\]*)*'",
    _litprefix + r'"[^\\n"\\]*(?:\\.[^\\n"\\]*)*"'
)
Operator: str = group(r'\*\*=?', r'>>=?', r'<<=?', r'<>', r'!=', r'//=?', r'->', r'[+\-*/%&@|^=<>]=?', r'~')
Bracket: str = r'[][(){}]'
Special: str = group(r'\r?\n', r':=', r'[:;.,`@]')
Funny: str = group(Operator, Bracket, Special)
PlainToken: str = group(Number, Funny, String, Name)
Token: str = Ignore + PlainToken
ContStr: str = group(
    (_litprefix + r"'[^\\n'\\]*(?:\\.[^\\n'\\]*)*") + group("'", r'\\\r?\n'),
    (_litprefix + r'"[^\\n"\\]*(?:\\.[^\\n"\\]*)*') + group('"', r'\\\r?\n')
)
PseudoExtras: str = group(r'\\\r?\n', Comment, Triple)
PseudoToken: str = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)
tokenprog, pseudoprog, single3prog, double3prog = map(re.compile, (Token, PseudoToken, Single3, Double3))
_strprefixes: Set[str] = (
    (_combinations('r', 'R', 'f', 'F') | _combinations('r', 'R', 'b', 'B'))
    | {'u', 'U', 'ur', 'uR', 'Ur', 'UR'}
)
endprogs: dict = {
    "'": re.compile(Single),
    '"': re.compile(Double),
    "'''": single3prog,
    '"""': double3prog,
    **{f"{prefix}'''": single3prog for prefix in _strprefixes},
    **{f'{prefix}"""': double3prog for prefix in _strprefixes},
    **{prefix: None for prefix in _strprefixes},
}
triple_quoted: Set[str] = (
    {"'''", '"""'}
    | {f"{prefix}'''" for prefix in _strprefixes}
    | {f'{prefix}"""' for prefix in _strprefixes}
)
single_quoted: Set[str] = (
    {"'", '"'}
    | {f"{prefix}'" for prefix in _strprefixes}
    | {f'{prefix}"' for prefix in _strprefixes}
)
tabsize: int = 8

class TokenError(Exception):
    def __init__(self, msg: str, error_details: Tuple[int, int]) -> None:
        super().__init__(msg)
        self.error_details = error_details

class StopTokenizing(Exception):
    pass

def printtoken(
    type: int,
    token: str,
    start: Tuple[int, int],
    end: Tuple[int, int],
    line: str
) -> None:
    srow, scol = start
    erow, ecol = end
    print(f'{srow},{scol}-{erow},{ecol}:\t{tok_name[type]}\t{repr(token)}')

def tokenize(
    readline: Callable[[], str],
    tokeneater: Callable[[int, str, Tuple[int, int], Tuple[int, int], str], None] = printtoken
) -> None:
    """
    The tokenize() function accepts two parameters: one representing the
    input stream, and one providing an output mechanism for tokenize().

    The first parameter, readline, must be a callable object which provides
    the same interface as the readline() method of built-in file objects.
    Each call to the function should return one line of input as a string.

    The second parameter, tokeneater, must also be a callable object. It is
    called once for each token, with five arguments, corresponding to the
    tuples generated by generate_tokens().
    """
    try:
        tokenize_loop(readline, tokeneater)
    except StopTokenizing:
        pass

def tokenize_loop(
    readline: Callable[[], str],
    tokeneater: Callable[[int, str, Tuple[int, int], Tuple[int, int], str], None]
) -> None:
    for token_info in generate_tokens(readline):
        tokeneater(*token_info)

class Untokenizer:
    def __init__(self) -> None:
        self.tokens: List[str] = []
        self.prev_row: int = 1
        self.prev_col: int = 0

    def add_whitespace(self, start: Tuple[int, int]) -> None:
        row, col = start
        assert row <= self.prev_row
        col_offset: int = col - self.prev_col
        if col_offset:
            self.tokens.append(' ' * col_offset)

    def untokenize(self, iterable: Iterator[Tuple[int, str, Tuple[int, int], Tuple[int, int], str]]) -> str:
        for t in iterable:
            if len(t) == 2:
                self.compat(t, iterable)
                break
            tok_type, token, start, end, line = t
            self.add_whitespace(start)
            self.tokens.append(token)
            self.prev_row, self.prev_col = end
            if tok_type in (NEWLINE, NL):
                self.prev_row += 1
                self.prev_col = 0
        return ''.join(self.tokens)

    def compat(
        self,
        token_item: Tuple[int, str],
        iterable: Iterator[Tuple[int, str, Tuple[int, int], Tuple[int, int], str]]
    ) -> None:
        startline: bool = False
        indents: List[str] = []
        toks_append = self.tokens.append
        toknum, tokval = token_item
        if toknum in (NAME, NUMBER):
            tokval += ' '
        if toknum in (NEWLINE, NL):
            startline = True
        for tok in iterable:
            toknum, tokval = tok[:2]
            if toknum in (NAME, NUMBER, ASYNC, AWAIT):
                tokval += ' '
            if toknum == INDENT:
                indents.append(tokval)
                continue
            elif toknum == DEDENT:
                indents.pop()
                continue
            elif toknum in (NEWLINE, NL):
                startline = True
            elif startline and indents:
                toks_append(indents[-1])
                startline = False
            toks_append(tokval)

cookie_re: re.Pattern = re.compile(r'^[ \t\f]*#.*?coding[:=][ \t]*([-\\w.]+)', re.ASCII)
blank_re: re.Pattern = re.compile(br'^[ \t\f]*(?:[#\r\n]|$)', re.ASCII)

def _get_normal_name(orig_enc: str) -> str:
    'Imitates get_normal_name in tokenizer.c.'
    enc: str = orig_enc[:12].lower().replace('_', '-')
    if enc == 'utf-8' or enc.startswith('utf-8-'):
        return 'utf-8'
    if (
        enc in ('latin-1', 'iso-8859-1', 'iso-latin-1')
        or enc.startswith(('latin-1-', 'iso-8859-1-', 'iso-latin-1-'))
    ):
        return 'iso-8859-1'
    return orig_enc

def detect_encoding(
    readline: Callable[[], bytes]
) -> Tuple[str, List[bytes]]:
    """
    The detect_encoding() function is used to detect the encoding that should
    be used to decode a Python source file. It requires one argument, readline,
    in the same way as the tokenize() generator.

    It will call readline a maximum of twice, and return the encoding used
    (as a string) and a list of any lines (left as bytes) it has read
    in.

    It detects the encoding from the presence of a utf-8 bom or an encoding
    cookie as specified in pep-0263. If both a bom and a cookie are present, but
    disagree, a SyntaxError will be raised. If the encoding cookie is an invalid
    charset, raise a SyntaxError.  Note that if a utf-8 bom is found,
    'utf-8-sig' is returned.

    If no encoding is specified, then the default of 'utf-8' will be returned.
    """
    bom_found: bool = False
    encoding: Optional[str] = None
    default: str = 'utf-8'

    def read_or_stop() -> bytes:
        try:
            return readline()
        except StopIteration:
            return bytes()

    def find_cookie(line: bytes) -> Optional[str]:
        nonlocal bom_found, encoding
        try:
            line_string = line.decode('ascii')
        except UnicodeDecodeError:
            return None
        match = cookie_re.match(line_string)
        if not match:
            return None
        encoding_candidate = _get_normal_name(match.group(1))
        try:
            codec = lookup(encoding_candidate)
        except LookupError:
            raise SyntaxError(f'unknown encoding: {encoding_candidate}')
        if bom_found:
            if codec.name != 'utf-8':
                raise SyntaxError('encoding problem: utf-8')
            return encoding_candidate + '-sig'
        return encoding_candidate

    first: bytes = read_or_stop()
    if first.startswith(BOM_UTF8):
        bom_found = True
        first = first[3:]
        default = 'utf-8-sig'
    if not first:
        return (default, [])
    encoding = find_cookie(first)
    if encoding:
        return (encoding, [first])
    if not blank_re.match(first):
        return (default, [first])
    second: bytes = read_or_stop()
    if not second:
        return (default, [first])
    encoding = find_cookie(second)
    if encoding:
        return (encoding, [first, second])
    return (default, [first, second])

def untokenize(iterable: Iterator[Tuple[int, str, Tuple[int, int], Tuple[int, int], str]]) -> str:
    """
    Transform tokens back into Python source code.

    Each element returned by the iterable must be a token sequence
    with at least two elements, a token number and token value.  If
    only two tokens are passed, the resulting output is poor.

    Round-trip invariant for full input:
        Untokenized source will match input source exactly

    Round-trip invariant for limited input:
        # Output text will tokenize the back to the input
        t1 = [tok[:2] for tok in generate_tokens(f.readline)]
        newcode = untokenize(t1)
        readline = iter(newcode.splitlines(1)).__next__
        t2 = [tok[:2] for tok in generate_tokens(readline)]
        assert t1 == t2
    """
    ut = Untokenizer()
    return ut.untokenize(iterable)

def generate_tokens(
    readline: Callable[[], str]
) -> Generator[Tuple[int, str, Tuple[int, int], Tuple[int, int], str], None, None]:
    """
    The generate_tokens() generator requires one argument, readline, which
    must be a callable object which provides the same interface as the
    readline() method of built-in file objects. Each call to the function
    should return one line of input as a string.  Alternately, readline
    can be a callable function terminating with StopIteration:
        readline = open(myfile).next    # Example of alternate readline

    The generator produces 5-tuples with these members: the token type; the
    token string; a 2-tuple (srow, scol) of ints specifying the row and
    column where the token begins in the source; a 2-tuple (erow, ecol) of
    ints specifying the row and column where the token ends in the source;
    and the line on which the token was found. The line passed is the
    physical line.
    """
    lnum: int = 0
    parenlev: int = 0
    continued: int = 0
    contstr: str = ''
    needcont: int = 0
    contline: Optional[str] = None
    indents: List[int] = [0]
    stashed: Optional[Tuple[int, str, Tuple[int, int], Tuple[int, int], str]] = None
    async_def: bool = False
    async_def_indent: int = 0
    async_def_nl: bool = False

    while True:
        try:
            line = readline()
        except StopIteration:
            line = ''
        lnum += 1
        pos: int = 0
        max_pos: int = len(line)
        if contstr:
            if not line:
                raise TokenError('EOF in multi-line string', strstart)
            endmatch = endprog.match(line)
            if endmatch:
                pos = end = endmatch.end(0)
                yield (STRING, contstr + line[:end], strstart, (lnum, end), contline + line)
                contstr = ''
                needcont = 0
            elif needcont and not (line.endswith('\\\n') or line.endswith('\\\r\n')):
                yield (ERRORTOKEN, contstr + line, strstart, (lnum, len(line)), contline)
                contstr = ''
                contline = None
                continue
            else:
                contstr += line
                contline += line
                continue
        elif parenlev == 0 and not continued:
            if not line:
                break
            column: int = 0
            while pos < max_pos:
                if line[pos] == ' ':
                    column += 1
                elif line[pos] == '\t':
                    column = (column // tabsize + 1) * tabsize
                elif line[pos] == '\f':
                    column = 0
                else:
                    break
                pos += 1
            if pos == max_pos:
                break
            if stashed:
                yield stashed
                stashed = None
            if line[pos] in '#\r\n':
                if line[pos] == '#':
                    comment_token: str = line[pos:].rstrip('\r\n')
                    nl_pos: int = pos + len(comment_token)
                    yield (COMMENT, comment_token, (lnum, pos), (lnum, pos + len(comment_token)), line)
                    yield (NL, line[nl_pos:], (lnum, nl_pos), (lnum, len(line)), line)
                else:
                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:], (lnum, pos), (lnum, len(line)), line)
                continue
            if column > indents[-1]:
                indents.append(column)
                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)
            while column < indents[-1]:
                if column not in indents:
                    raise IndentationError(
                        'unindent does not match any outer indentation level',
                        ('<tokenize>', lnum, pos, line)
                    )
                indents.pop()
                if async_def and async_def_indent >= indents[-1]:
                    async_def = False
                    async_def_nl = False
                    async_def_indent = 0
                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)
            if async_def and async_def_nl and async_def_indent >= indents[-1]:
                async_def = False
                async_def_nl = False
                async_def_indent = 0
        else:
            if not line:
                raise TokenError('EOF in multi-line statement', (lnum, 0))
            continued = 0

        while pos < max_pos:
            pseudomatch = pseudoprog.match(line, pos)
            if pseudomatch:
                start, end = pseudomatch.span(1)
                spos: Tuple[int, int] = (lnum, start)
                epos: Tuple[int, int] = (lnum, end)
                pos = end
                token_str: str = line[start:end]
                initial: str = line[start]
                if initial in string.digits or (initial == '.' and token_str != '.'):
                    yield (NUMBER, token_str, spos, epos, line)
                elif initial in '\r\n':
                    newline = NEWLINE if parenlev == 0 and not async_def else NL
                    if async_def:
                        async_def_nl = True
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (newline, token_str, spos, epos, line)
                elif initial == '#':
                    assert not token_str.endswith('\n')
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (COMMENT, token_str, spos, epos, line)
                elif token_str in triple_quoted:
                    endprog = endprogs.get(token_str)
                    endmatch = endprog.match(line, pos) if endprog else None
                    if endmatch:
                        pos = endmatch.end(0)
                        token_full: str = line[start:pos]
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token_full, spos, (lnum, pos), line)
                    else:
                        strstart: Tuple[int, int] = (lnum, start)
                        contstr = line[start:]
                        needcont = 1
                        contline = line
                        break
                elif initial in single_quoted or token_str[:2] in single_quoted or token_str[:3] in single_quoted:
                    if token_str.endswith('\n'):
                        strstart = (lnum, start)
                        endprog = endprogs.get(initial) or endprogs.get(token_str[1]) or endprogs.get(token_str[2])
                        contstr = line[start:]
                        needcont = 1
                        contline = line
                        break
                    else:
                        if stashed:
                            yield stashed
                            stashed = None
                        yield (STRING, token_str, spos, epos, line)
                elif initial.isidentifier():
                    if token_str in ('async', 'await') and async_def:
                        yield ((ASYNC if token_str == 'async' else AWAIT), token_str, spos, epos, line)
                        continue
                    tok: Tuple[int, str, Tuple[int, int], Tuple[int, int], str] = (NAME, token_str, spos, epos, line)
                    if token_str == 'async' and not stashed:
                        stashed = tok
                        continue
                    if token_str in ('def', 'for') and stashed and stashed[0] == NAME and stashed[1] == 'async':
                        if token_str == 'def':
                            async_def = True
                            async_def_indent = indents[-1]
                        yield (ASYNC, stashed[1], stashed[2], stashed[3], stashed[4])
                        stashed = None
                    if stashed:
                        yield stashed
                        stashed = None
                    yield tok
                elif initial == '\\':
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (NL, token_str, spos, epos, line)
                    continued = 1
                else:
                    if initial in '([{':
                        parenlev += 1
                    elif initial in ')]}':
                        parenlev -= 1
                    if stashed:
                        yield stashed
                        stashed = None
                    yield (OP, token_str, spos, epos, line)
            else:
                yield (ERRORTOKEN, line[pos], (lnum, pos), (lnum, pos + 1), line)
                pos += 1

    if stashed:
        yield stashed
        stashed = None
    for indent in indents[1:]:
        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')
    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')

if __name__ == '__main__':
    import sys
    from typing import Callable

    if len(sys.argv) > 1:
        file_readline: Callable[[], str] = open(sys.argv[1]).readline
        tokenize(file_readline)
    else:
        tokenize(sys.stdin.readline)
